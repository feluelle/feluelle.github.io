- layout: left
  company: Trade Republic Bank GmbH
  link: https://traderepublic.com/
  job_title: Data Engineer
  dates: November 2019 - Present
  quote: >
   "Make your money work for you." - Trade Republic Bank GmbH
  description: |
   For Trade Republic I am..
   * building a data platform on aws from scratch with terraform, kubernetes (eks, fargate), snowflake, airflow (mwaa), dbt, dms, and more
   * maintaining airflow pipelines for data ingestion
   * onboarding data scientists and analytic engineers to the data platform
   * scaling services to handle the rapidly growing organization from serving 7 to serving about 150 engineers
   * planning, coordinating and implementing the migration from self-hosted Airflow to MWAA
   * building an Airflow PR development workflow on a remote airflow instance that scales, handles 100s of engineers developing concurrently

   **Scaling Users ðŸ§‘â€ðŸ”¬. Scaling Tools & Services ðŸ› . Scaling Infrastructure ðŸ¡.**
- layout: left
  company: Apache Software Foundation
  link: https://www.apache.org/
  job_title: Apache Airflow Committer
  dates: September 2019 - Present
  quote: >
   "The World's Largest Open Source Foundation" - Apache Software Foundation
  description: |
   For Apache Airflow I am..
   * writing project and code documentation
   * writing unit tests in python
   * adding new features like Airflow hooks and operators
   * reporting bugs via Jira or GitHub Issues
   * communicating with the community via email and slack
   * writing Airflow Improvement Proposals in Confluence
   * reviewing GitHub pull requests
   * helping users to join the community
   * testing and voting releases

   **Open Source is Love â¤ï¸. Communication is Key ðŸ’¬. Commitment ðŸ”.**
- layout: left
  company: Digitas Pixelpark GmbH
  link: https://www.digitaspixelpark.com/
  job_title: Junior Data Engineer
  dates: January 2018 - November 2020
  quote: >
   "Germany's most impactful customer experience agency" - Digitas Pixelpark GmbH
  description: |
   * built a data management platform on AWS for our data analysts and scientists to access analytics data
   efficiently
   * added CI/CD to our DMP (data management platform) via GitLab CI/CD which lints, tests, builds documentation
   and deploys our code to a development or production environment
   * built (ETL/ELT) data pipelines with Apache Airflow
   * connected new data sources (mostly via REST APIs) to our data platform
   * transformed data via SQL or Python Pandas to be analytics-ready
   * designed workflows efficiently by making use of a lot of features of Apache Airflow
   * monitored of our data pipelines

   **Containers are awesome ðŸ³. Automatisation ðŸ”„. Cloud Services â˜ï¸. Leading ðŸ‘¨â€ðŸ«.**
- layout: left
  company: NETRONIC Software GmbH
  link: https://www.netronic.com/
  job_title: Student Research Assistant
  dates: August 2012 - August 2016
  quote: >
   "Our mission is to enable every SMB organization to achieve operational agility with visual scheduling." - NETRONIC Software GmbH
  description: |
   * built a .NET Windows Forms application in C#
   * learned that code readibility is important through refactoring and mentorship
   * learned the efficient usage of version control systems
   * built a web application accessing Microsoft Azure Active Directoy via a REST API to manage users for one
   of our products
   * built a hybrid mobile application to let our customers monitor their machines status easily

   **Code can be elegant ðŸ’Ž. Teamwork ðŸ‘ª. Coding ðŸ’».**
